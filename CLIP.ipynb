{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zF5Jv--gHnzV",
        "outputId": "42ededee-874c-4ce2-f0bf-b6ea9b4e1450"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: conda: command not found\n",
            "Collecting ftfy\n",
            "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.1)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.10/dist-packages (from ftfy) (0.2.6)\n",
            "Installing collected packages: ftfy\n",
            "Successfully installed ftfy-6.1.1\n",
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-x9t58sr3\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-x9t58sr3\n",
            "  Resolved https://github.com/openai/CLIP.git to commit a1d071733d7111c9c014f024669f959182114e33\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (6.1.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (4.66.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2.0.1+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (0.15.2+cu118)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.10/dist-packages (from ftfy->clip==1.0) (0.2.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->clip==1.0) (3.27.4.1)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->clip==1.0) (16.0.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (1.23.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (2.31.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->clip==1.0) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->clip==1.0) (1.3.0)\n",
            "Building wheels for collected packages: clip\n",
            "  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369497 sha256=d4c207313136f7af05fd3bca2b542da9ab0e99a196bba17a9f9560f525189d58\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-d3a905ch/wheels/da/2b/4c/d6691fa9597aac8bb85d2ac13b112deb897d5b50f5ad9a37e4\n",
            "Successfully built clip\n",
            "Installing collected packages: clip\n",
            "Successfully installed clip-1.0\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.33.2-py3-none-any.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Collecting huggingface-hub<1.0,>=0.15.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.17.1-py3-none-any.whl (294 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.8/294.8 kB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m48.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m34.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.17.1 safetensors-0.3.3 tokenizers-0.13.3 transformers-4.33.2\n"
          ]
        }
      ],
      "source": [
        "! conda install --yes -c pytorch pytorch=1.7.1 torchvision cudatoolkit=11.0\n",
        "! pip install ftfy regex tqdm\n",
        "! pip install git+https://github.com/openai/CLIP.git\n",
        "! pip install transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# First Task"
      ],
      "metadata": {
        "id": "1v7HwailZthu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import clip\n",
        "import torch\n",
        "import pandas as pd\n",
        "\n",
        "# Load the model\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model, preprocess = clip.load('ViT-B/32', device)\n",
        "\n",
        "csv_path = '/content/document.csv'\n",
        "df = pd.read_csv(csv_path)\n",
        "\n",
        "def best_docu(text_query):\n",
        "\n",
        "  q_tokens = clip.tokenize(text_query)\n",
        "  q_feature = model.encode_text(q_tokens)\n",
        "\n",
        "  values = []\n",
        "\n",
        "  for text in df.text.values:\n",
        "\n",
        "    # all CLIP models use 77 as the context length that's why I had to use truncate here.\n",
        "    # I looked in the huggingface. I probably have to retrain the whole model to increase the context length.\n",
        "    text_input = clip.tokenize(text,context_length=77, truncate=True).to(device)\n",
        "\n",
        "  # Calculate features\n",
        "    with torch.no_grad():\n",
        "        text_features = model.encode_text(text_input)\n",
        "\n",
        "    # calculating similarity of each document\n",
        "    q_feature /= q_feature.norm(dim=-1,keepdim=True)\n",
        "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "    similarity = q_feature @ text_features.T\n",
        "\n",
        "    values.append(similarity)\n",
        "\n",
        "  index = torch.argmax(torch.tensor(values)).item()\n",
        "\n",
        "  return index"
      ],
      "metadata": {
        "id": "7Ko1fLWYH148",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66c71dab-cf45-41f1-dbec-1a6451045165"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|███████████████████████████████████████| 338M/338M [00:05<00:00, 62.6MiB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Second Task"
      ],
      "metadata": {
        "id": "IGus2MpEZy4q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import os\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "import numpy\n",
        "\n",
        "# Specify the folder path containing the images\n",
        "folder_path = '/content/Images'\n",
        "\n",
        "imgs = []\n",
        "# Check if the folder exists\n",
        "if os.path.exists(folder_path):\n",
        "    # Get a list of all files in the folder\n",
        "    file_list = os.listdir(folder_path)\n",
        "\n",
        "    # Loop through the files and read each image\n",
        "    for file_name in file_list:\n",
        "        # Construct the full path to the image file\n",
        "        image_path = os.path.join(folder_path, file_name)\n",
        "\n",
        "        if file_name.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp', '.gif')):\n",
        "            # Open the image using PIL\n",
        "            imgs.append((preprocess(Image.open(image_path)).unsqueeze(0).squeeze().to(device),file_name))\n",
        "else:\n",
        "    print(f\"The folder '{folder_path}' does not exist.\")\n",
        "\n",
        "\n",
        "def get_features(dataset):\n",
        "    all_features = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, _ in tqdm(DataLoader(dataset, batch_size=100,shuffle=False)):\n",
        "            features = model.encode_image(images.to(device))\n",
        "\n",
        "            all_features.append(features)\n",
        "\n",
        "    return torch.cat(all_features)\n",
        "\n",
        "\n",
        "def best_images(text_query):\n",
        "\n",
        "  features = get_features(imgs)\n",
        "  q_tokens = clip.tokenize(text_query)\n",
        "  q_feature = model.encode_text(q_tokens)\n",
        "\n",
        "\n",
        "  # Pick the top 5 most similar labels for the image\n",
        "  q_feature /= q_feature.norm(dim=-1, keepdim=True)\n",
        "  features /= features.norm(dim=-1, keepdim=True)\n",
        "  similarity = (100.0 * q_feature @ features.T).softmax(dim=-1)\n",
        "  values, indices = similarity[0].topk(20)\n",
        "  return indices"
      ],
      "metadata": {
        "id": "qgFcpkA8cWDG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## output File function"
      ],
      "metadata": {
        "id": "2BDn1p2mtqHJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import os\n",
        "\n",
        "def write_prediction_csv(output_file, predictions):\n",
        "\n",
        "    # Define the column names\n",
        "    fieldnames = ['qid', 'doc_id', 'img_id']\n",
        "\n",
        "    # Open the CSV file in write mode\n",
        "    with open(output_file, mode='a', newline='') as csvfile:\n",
        "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "\n",
        "        # Write the header row\n",
        "        writer.writeheader()\n",
        "\n",
        "        # Write the prediction data\n",
        "        for prediction in predictions:\n",
        "            writer.writerow(prediction)\n",
        "\n"
      ],
      "metadata": {
        "id": "rjQZB0CKtsrF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference"
      ],
      "metadata": {
        "id": "yl-dMriJZWiR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_query = 'confront your friend about the abuse.'\n",
        "\n",
        "index = best_docu(text_query) #returns the index of the best documents\n",
        "indices = best_images(text_query) #retruns top 20 indices of the images\n",
        "\n",
        "# Writing the file\n",
        "predictions = [\n",
        "    {'qid':text_query,'doc_id': df.doc_id[index], 'img_id': [imgs[image_ID][1] for image_ID in indices[:5]]},\n",
        "]\n",
        "write_prediction_csv('predictions.csv', predictions)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bBpuc8oesorG",
        "outputId": "5a69ace7-71d1-48c3-d0eb-e365670e4266"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [00:37<00:00, 12.35s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6JegNDoPxYtV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}